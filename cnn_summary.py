# -*- coding: utf-8 -*-
"""cnn summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNsa0nPunoH8HJhConfXxRLTePuCgag-

During experimentation, I observed that the ROUGE score of the fine-tuned T5 summarization model consistently plateaued around 37, despite multiple training attempts. This behavior suggests a limitation of the model–metric setup rather than a deficiency in summary quality.
Since ROUGE, while not fully representative of semantic quality, is still used as a primary evaluation indicator, I explored possible strategies to improve robustness. Inspired by ensemble methods commonly reported in the text summarization literature, I proposed an ensemble-based approach combining T5 and BART.
Due to hardware memory constraints, a full ensemble implementation was not feasible. Instead, I implemented a proof of concept on a single news article to validate the idea. Rather than selecting summaries based on ROUGE scores—which is computationally expensive and impractical during inference—I adopted a length-based selection heuristic, choosing the longer summary under the assumption that longer outputs preserve more semantic content and tend to correlate with higher ROUGE overlap.
This approach was designed as a memory-efficient and scalable alternative, allowing future experimentation if additional computational resources become available, while maintaining a balance between performance and efficiency.
"""

!pip install -q transformers datasets evaluate rouge_score accelerate

!pip install transformers datasets

!pip install accelerate -U

import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"


!pip install transformers[torch]

!pip install rouge

from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments

from datasets import load_dataset

dataset = load_dataset("cnn_dailymail", "3.0.0")


model_name = "t5-base"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

dataset

def preprocess_function(examples):

   inputs = [doc for doc in examples['article']]

   model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")


   labels = tokenizer(examples['highlights'], max_length=128, truncation=True, padding="max_length")

   model_inputs["labels"] = labels["input_ids"]

   return model_inputs

encoded_dataset = dataset.map(preprocess_function, batched=True)

train_dataset = encoded_dataset["train"].shuffle(seed=42).select(range(2000))

test_dataset = encoded_dataset["validation"].shuffle(seed=42).select(range(1000))

from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=3e-4,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=3,
    report_to="none"
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,

)

# Train the model
trainer.train()

trainer.evaluate()

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

def generate_summary_batch(batch):
   with torch.no_grad():
    input_ids = tokenizer(batch["article"], padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    output = model.generate(
        input_ids["input_ids"],
        max_length=150,
        num_beams=5,
        temperature=0.7,
        top_k=50,
        top_p=0.95,
        do_sample=True,
        early_stopping=True
    )

    summaries = tokenizer.batch_decode(output, skip_special_tokens=True)
    torch.cuda.empty_cache()

    return {"summary": summaries}

summaries = test_dataset.map(generate_summary_batch, batched=True, batch_size=8)

from rouge import Rouge

def calculate_rouge(reference_list,generated_list):
    rouge=Rouge()
    scores=rouge.get_scores(generated_list,reference_list)
    rouge_1=sum(score['rouge-1']['f'] for score in scores)/len(scores)
    rouge_2=sum(score['rouge-2']['f'] for score in scores)/len(scores)
    rouge_l=sum(score['rouge-l']['f'] for score in scores)/len(scores)
    return rouge_1,rouge_2,rouge_l

# Initialize lists to store reference and generated summaries

reference_summaries = [example["highlights"] for example in test_dataset]
generated_summaries = [example["summary"] for example in summaries]

# Calculate ROUGE scores

rouge_1, rouge_2, rouge_l = calculate_rouge(reference_summaries,generated_summaries)

print("Average ROUGE-1:", rouge_1)
print("Average ROUGE-2:", rouge_2)

print("Average ROUGE-L:", rouge_l)

from transformers import T5Tokenizer, T5ForConditionalGeneration, BartForConditionalGeneration, BartTokenizer

original_text = """
Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed.
The Australia international full-back or centre quit the National Rugby League in October to try his luck in American football
and was this week given a three-year contract with the San Francisco 49ers.
Peter Illfield, chairman of US Association of Rugby League, said: 'Jarryd, at 27, is one of the most gifted and talented rugby league players in Australia.
He is an extraordinary athlete. His three-year deal with the 49ers, as an expected running back, gives the USA Rugby League a connection with the American football lover like never before.'
"""
t5_model_name = "t5-small"
t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)
t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)

bart_model_name = "facebook/bart-large-cnn"
bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)
bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)

def generate_t5_summary(text):
    inputs = t5_tokenizer("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = t5_model.generate(inputs["input_ids"], max_length=128, num_beams=4, early_stopping=True)
    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def generate_bart_summary(text):
    inputs = bart_tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = bart_model.generate(inputs["input_ids"], max_length=128, num_beams=4, early_stopping=True)
    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def ensemble_summary(text):
    t5_summary = generate_t5_summary(text)

    bart_summary = generate_bart_summary(text)

    return t5_summary if len(t5_summary) > len(bart_summary) else bart_summary

ensemble_summary_text = ensemble_summary(original_text)
print("Ensemble Summary:", ensemble_summary_text)

pip install gensim

pip install gensim

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
from gensim import corpora, models, similarities
from gensim.parsing.preprocessing import preprocess_string

def gensim_textrank_summarizer(text, top_n=3):

    try:
        sentences = text.split('. ')
        if len(sentences) <= top_n: return text

        processed_sentences = [preprocess_string(s) for s in sentences]

        dictionary = corpora.Dictionary(processed_sentences)
        corpus = [dictionary.doc2bow(doc) for doc in processed_sentences]

        tfidf = models.TfidfModel(corpus)
        index = similarities.MatrixSimilarity(tfidf[corpus],
    num_features=len(dictionary))

        sentence_ranks = []
        for i in range(len(corpus)):
            sims = index[tfidf[corpus[i]]]
            sentence_ranks.append(sum(sims))

        top_indices = sorted(range(len(sentence_ranks)), key=lambda i: sentence_ranks[i], reverse=True)[:top_n]
        top_indices.sort()

        summary = ". ".join([sentences[i].strip() for i in top_indices])
        return summary + "."
    except Exception as e:
        return text[:300]

dataset_subset = dataset['validation'].select(range(1000))

def apply_gensim(example):
    try:
        example['gensim_extractive'] = gensim_textrank_summarizer(example['article'])
    except:
        example['gensim_extractive'] = ""
    return example

dataset_subset = dataset_subset.map(apply_gensim)

import pandas as pd

df = pd.DataFrame(dataset_subset)

display_df = df[['article', 'highlights', 'gensim_extractive']].head(50)


display_df